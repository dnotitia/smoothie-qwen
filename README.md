# Smoothie Qwen
<p align="center">
    <img src="asset/smoothie-qwen-logo.png" width="400">
</p>

**Smoothie Qwen** is a lightweight adjustment tool that smooths token probabilities in Qwen and similar models, enhancing balanced multilingual generation capabilities. We've uploaded pre-adjusted models to our [Smoothie Qwen Collection on ğŸ¤— Hugging Face](https://huggingface.co/collections/dnotitia/smoothie-qwen3-6811896ebb3a255de7b5b437) for your convenience:

<details open>
<summary><b>Smoothie-Qwen3 Collection</b> (click to expand)</summary>

- [dnotitia/Smoothie-Qwen3-0.6B](https://huggingface.co/dnotitia/Smoothie-Qwen3-0.6B)
- [dnotitia/Smoothie-Qwen3-1.7B](https://huggingface.co/dnotitia/Smoothie-Qwen3-1.7B)
- [dnotitia/Smoothie-Qwen3-4B](https://huggingface.co/dnotitia/Smoothie-Qwen3-4B)
- [dnotitia/Smoothie-Qwen3-8B](https://huggingface.co/dnotitia/Smoothie-Qwen3-8B)
- [dnotitia/Smoothie-Qwen3-14B](https://huggingface.co/dnotitia/Smoothie-Qwen3-14B)
- [dnotitia/Smoothie-Qwen3-32B](https://huggingface.co/dnotitia/Smoothie-Qwen3-32B)
- [dnotitia/Smoothie-Qwen3-30B-A3B](https://huggingface.co/dnotitia/Smoothie-Qwen3-30B-A3B)
- [dnotitia/Smoothie-Qwen3-235B-A22B](https://huggingface.co/dnotitia/Smoothie-Qwen3-235B-A22B)

</details>

<details>
<summary><b>Smoothie-Qwen2.5 Collection</b> (click to expand)</summary>

- [dnotitia/Smoothie-Qwen2.5-0.5B-Instruct](https://huggingface.co/dnotitia/Smoothie-Qwen2.5-0.5B-Instruct)
- [dnotitia/Smoothie-Qwen2.5-1.5B-Instruct](https://huggingface.co/dnotitia/Smoothie-Qwen2.5-1.5B-Instruct)
- [dnotitia/Smoothie-Qwen2.5-3B-Instruct](https://huggingface.co/dnotitia/Smoothie-Qwen2.5-3B-Instruct)
- [dnotitia/Smoothie-Qwen2.5-7B-Instruct](https://huggingface.co/dnotitia/Smoothie-Qwen2.5-7B-Instruct)
- [dnotitia/Smoothie-Qwen2.5-14B-Instruct](https://huggingface.co/dnotitia/Smoothie-Qwen2.5-14B-Instruct)
- [dnotitia/Smoothie-Qwen2.5-32B-Instruct](https://huggingface.co/dnotitia/Smoothie-Qwen2.5-32B-Instruct)
- [dnotitia/Smoothie-Qwen2.5-72B-Instruct](https://huggingface.co/dnotitia/Smoothie-Qwen2.5-72B-Instruct)

</details>

These pre-smoothed models are ready for immediate integration into your projects.

## Contents
- [Overview](#overview)
- [Key Features](#key-features)
- [Installation](#installation)
- [Usage](#usage)
- [Parameters](#parameters)
- [How It Works](#how-it-works)
- [Notes](#notes)
- [References](#references)

## Overview

**Smoothie Qwen** is a post-processing tool designed to subtly refine the token distribution in Qwen3 & Qwen2.5 models. By analyzing and adjusting token weights particularly those associated with specific Unicode ranges it helps mitigate unintended biases toward certain languages while preserving the modelâ€™s core capabilities.

This approach is especially useful for applications requiring balanced multilingual outputs, where overrepresentation of one language might skew results. The tool identifies target tokens through Unicode ranges, including subword tokenization (e.g., partial characters from BPE tokenization), and applies probabilistic smoothing to encourage diversity.

## Key Features

- Token identification based on Unicode ranges of the target language
- Detection of broken or malformed tokens (e.g. `ï¿½`) caused by subword tokenization
- Identification of token combinations that may probabilistically form the target language
- Flexible analysis strategies (e.g., N-gram analysis) to detect high-risk token patterns
- Configurable analysis methods with future support for additional techniques
- Adjustment of token weights in the `lm_head` layer to reduce generation likelihood
- Saving of modified models for reuse or deployment
- Automation of model generation across parameter variations (`min_scale`, `smoothness`)


## Installation

Clone the repository and navigate into the project directory:

```shell
git clone https://github.com/dnotitia/smoothie-qwen.git
cd smoothie-qwen
```

Create a virtual environment and install the dependencies using `uv`:

```shell
uv venv
source .venv/bin/activate
uv pip install -r requirements.txt
```

> If `uv` is not installed, follow the [installation guide here](https://github.com/astral-sh/uv).


## Usage

Run with a YAML configuration file:
```shell
$ python src/main.py --config config.yaml
```

## Parameters
- `model.name`: Name or path of the base model to modify.
- `model.output_path`: Directory path to save the modified model (default: ./modified_model).
- `analysis.method`: Token analysis method (currently supports "ngram").
- `analysis.window_size`: Size of the window used for combining tokens during analysis. (default: 2, range: 2â€“4)
- `analysis.sample_size`: Number of samples to analyze per token (default: 1000).
- `analysis.ngram_weights`: List of weights used for n-gram analysis.
  - The number of weights used depends on window_size: use the first weight for size 2, the first two for size 3, and all three for size 4.
  - The weights are normalized automatically if they do not sum to 1.0.
  - Example: [1.0, 0.0, 0.0] will consider only bigram analysis.
- `adjustment.min_scale`: Minimum scaling factor applied to target tokens when fully suppressed (range: 0.0â€“1.0).
  - 1.0: No weight adjustment.
  - 0.1: The most suppressed target tokens will have their weights multiplied by 0.1.
  - Lower `min_scale` values allow stronger suppression of identified tokens.
- `adjustment.smoothness`: Controls the steepness of weight suppression based on token generation probability (>1).
  - Higher values cause the weight to decrease more sharply even at lower probabilities.
  - Lower values result in a more gradual and smoother suppression curve.
- `unicode_targets`: List of Unicode ranges specifying which language tokens to target.

## How It Works

<p align="center">
  <img src="asset/flow_chart.png" width="800">
</p>

1. **Token Identification**: Identify tokens in the target Unicode ranges, including broken or malformed tokens from subword tokenization (e.g., BPE artifacts).
2. **Token Combination Analysis**: Analyze token sequences using N-gram methods to detect combinations that are likely to produce the target language.
3. **Weight Smoothing**: Adjust (down-weight) the probabilities of the identified tokens in the `lm_head` layer based on the specified min_scale and smoothness parameters.
4. **Model Saving**: Save the model with updated token weights to a new directory for later use or deployment.

### Weight Smoothing Formula

The scale factor **S** applied to each token's weight is calculated as:

<p align="center">
  <img src="asset/formula.png" width="850">
</p>

<p align="center">
  <img src="asset/smoothing_function_plot.png" width="600">
</p>

where:
- `min_scale` defines the minimum weight scaling allowed (between 0 and 1).
- `smoothness` controls the sharpness of suppression (higher values = more aggressive).
- `weighted_prob` is the estimated probability of generating the target language (between 0 and 1).

The original token weight is multiplied by **S** to smoothly adjust the generation probability.

<p align="center">
  <img src="asset/weight_comparison.png" width="800">
</p>
<p align="center">
  <i>A 3D visualization comparing <code>lm_head</code> weights before (left) and after (right) the smoothing transformation. <br>In the modified model, the weights for target tokens are scaled towards zero, creating a distinctly "flattened" surface. This illustrates the targeted nature of the smoothing process.</i>
</p>

---

## Experiments
We conducted simple experiments with **Qwen2.5-Coder-14B-Instruct** to validate **Smoothie Qwen**'s effectiveness in suppressing unintended Chinese generation while maintaining core performance.

> **Note:**
> These tests used a minimal n-gram window size of 2, demonstrating that even basic token combination analysis achieves significant effects.

### Experimental Setup
- **Base Model**: `Qwen2.5-Coder-14B-Instruct`
- **Evaluation Tool**: Custom-modified [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)

### Evaluation Datasets & Metrics

We used two main datasets to evaluate our approach with the following metrics:

- **Custom Chinese Elicitation Dataset**:
  - Synthetic prompts designed to elicit Chinese translation
  - Measured with **chin_prom**: Chinese suppression rate on prompts deliberately designed to trigger Chinese responses

- [**KMMLU**](https://huggingface.co/datasets/HAERAE-HUB/KMMLU) (Korean-MMLU):
  - Korean benchmark covering Computer Science (CS) and Industrial Engineering (IE) domains
  - Measured with:
    - **chin_cs/chin_ie**: Chinese suppression rate in generated responses
    - **acc_cs/acc_ie**: Task accuracy (to verify performance maintenance)

**Scoring interpretation** for all chin_* metrics:
- **1.0**: Perfect (100% Chinese suppression)
- **0.0**: Poor (Chinese appears in all responses)
- Higher scores are better, indicating more effective Chinese suppression

---

### 1. Weight Adjustment Summary

- **Vocabulary size**: 151,643 tokens
- **Target tokens**: 26,153 tokens (17.25%)
- **Broken tokens**: 1,457 tokens (0.96%)

<p align="center">
  <img src="asset/token_weight_heatmap_2d.png" width="600">
</p>

<p align="center">
  <img src="asset/token_weight_heatmap_3d.png" width="600">
</p>

---

### 2. Experiment 1: min_scale Adjustment

The `min_scale` parameter controls the maximum reduction of token weights for identified Chinese tokens.

| min_scale | chin_prom | chin_cs | chin_ie | acc_cs | acc_ie |
|:---------:|:---------:|:-------:|:-------:|:------:|:------:|
| 1.0 (base)| 0.190     | 0.995   | 0.990   | 0.715  | 0.385  |
| 0.9       | 0.250     | 0.995   | 1.000   | 0.710  | 0.395  |
| 0.8       | 0.375     | 0.995   | 1.000   | 0.710  | 0.395  |
| 0.7       | 0.605     | 0.995   | 1.000   | 0.710  | 0.395  |
| 0.6       | 0.875     | 0.995   | 1.000   | 0.710  | 0.395  |
| 0.5       | 0.950     | 0.995   | 1.000   | 0.710  | 0.395  |
| 0.4       | 0.965     | 0.995   | 1.000   | 0.710  | 0.395  |
| 0.3       | 0.980     | 0.995   | 1.000   | 0.710  | 0.395  |
| 0.2       | 0.985     | 1.000   | 1.000   | 0.710  | 0.395  |
| 0.1       | 0.990     | 1.000   | 1.000   | 0.710  | 0.395  |

**Key Findings:**
- Lower `min_scale` values significantly improve Chinese suppression rate
- 95% suppression achieved with simple 2-gram analysis at `min_scale = 0.5`
- Task performance (`acc_cs`, `acc_ie`) remains stable across all settings
- **Reasonable trade-off**: `min_scale = 0.5`

<p align="center">
  <img src="asset/exp1_min_scale.png" width="600">
</p>

---

### 3. Experiment 2: smoothness Adjustment

The `smoothness` parameter controls the steepness of weight scaling applied to identified tokens.

| smoothness | chin_prom | chin_cs | chin_ie | acc_cs | acc_ie |
|:----------:|:---------:|:-------:|:-------:|:------:|:------:|
| 1.1        | 0.920     | 0.995   | 1.000   | 0.710  | 0.395  |
| 10.0       | 0.950     | 0.995   | 1.000   | 0.710  | 0.395  |
| 100.0      | 0.990     | 1.000   | 1.000   | 0.710  | 0.395  |
| 1000.0     | 1.000     | 1.000   | 1.000   | 0.710  | 0.395  |

**Key Findings:**
- Higher `smoothness` values enable stronger suppression
- Suppression on evaluation samples was fully achieved (1.000) at `smoothness = 1000.0`
- Task performance remains consistent across all settings
- Setting smoothness = 10.0 alone was enough to achieve satisfactory suppression performance

<p align="center">
  <img src="asset/exp2_smoothness.png" width="600">
</p>

### 4. Qualitative Examples: Original vs. Smoothed

We compared the outputs of the original and smoothed models using prompts intentionally designed to elicit Chinese generation.  
The smoothed model was adjusted using **Smoothie Qwen** with parameters:  
**`min_scale = 0.5`** and **`smoothness = 10.0`**.

---

**Prompt 1:**  
Please tell me the numbers 1 to 3 in Chinese.

**Original Model Response:**
```text
Sure! The numbers 1 to 3 in Chinese are:

1. ä¸€ (yÄ«)
2. äºŒ (Ã¨r)
3. ä¸‰ (sÄam)
```

**Smoothie Qwen Response:**
```text
Sure! Here are the numbers 1 to 3 in Chinese:

1 - (yÄ«)  
2 - (Ã¨r)  
3 - (sÄam)  
```

---

**Prompt 2:**  
ì£¼ì–´ì§„ í•œì ë¬¸ìì—´ì„ ìœ ë‹ˆì½”ë“œ í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” íŒŒì´ì¬ í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ì¤˜

**Original Model Response:**

í•œìë¥¼ í™•ì¸í•˜ëŠ” íŒŒì´ì¬ í•¨ìˆ˜ë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œìëŠ” ìœ ë‹ˆì½”ë“œ ë²”ìœ„ `'\u4e00'`ì—ì„œ `'\u9fff'` ì‚¬ì´ì˜ ë¬¸ìë¡œ ì •ì˜ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ í•œìì¸ì§€ íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” ê·¸ ì˜ˆì‹œ ì½”ë“œì…ë‹ˆë‹¤:

```python
def is_chinese_character(char):
   if len(char) != 1:
       return False
   return '\u4e00' <= char <= '\u9fff'

# í…ŒìŠ¤íŠ¸
print(is_chinese_character('í•œ'))  # False
print(is_chinese_character('ä¸­'))  # True
```

ì´ í•¨ìˆ˜ëŠ” ë‹¨ì¼ ë¬¸ìë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, í•´ë‹¹ ë¬¸ìê°€ í•œìì¸ì§€ boolean ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤. í•œìê°€ ì•„ë‹ˆë©´ `False`, í•œìì´ë©´ `True`ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

**Smoothie Qwen Response:**

ë‹¤ìŒì€ Pythonìœ¼ë¡œ ì‘ì„±ëœ ê·¸ëŸ¬í•œ í•¨ìˆ˜ì˜ ê°€ëŠ¥í•œ êµ¬í˜„ì…ë‹ˆë‹¤.
```python
def is_chinese_character(s):
   """
   ì£¼ì–´ì§„ ë¬¸ìì—´ì´ í•œì(ì¤‘êµ­ì–´)ë¥¼ ë‚˜íƒ€ë‚´ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

   Args:
       s (str): ì…ë ¥ ë¬¸ìì—´

   Returns:
       bool: ì…ë ¥ ë¬¸ìì—´ì´ í•œìë¥¼ ë‚˜íƒ€ë‚´ë©´ True, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ False
   """
   # ì…ë ¥ì´ ë¹„ì–´ ìˆìœ¼ë©´ Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
   if not s:
       return False
   
   # ì…ë ¥ ë¬¸ìì—´ì˜ ê° ë¬¸ìë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤.
   for char in s:
       # í˜„ì¬ ë¬¸ìê°€ ìœ ë‹ˆì½”ë“œ ë²”ìœ„ [U+4E00, U+9FFF] ë‚´ì— ì—†ìœ¼ë©´ Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
       if ord(char) < 0x4E00 or ord(char) > 0x9FFF:
           return False
       
   # ëª¨ë“  ë¬¸ìê°€ í•œìë¡œ ë³´ì´ë¯€ë¡œ Trueë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
   return True
```

ì´ í•¨ìˆ˜ëŠ” ì…ë ¥ ë¬¸ìì—´ `s`ì˜ ê° ë¬¸ìë¥¼ ë°˜ë³µí•˜ê³  ìœ ë‹ˆì½”ë“œ ì½”ë“œ í¬ì¸íŠ¸ê°€ ì¤‘êµ­ì–´ í•œìëŠ” í¬í•¨í•˜ëŠ” [U+4E00, U+9FFF] ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì…ë ¥ ë¬¸ìì—´ì´ ë¹„ì–´ ìˆìœ¼ë©´ Falseë¥¼ ë°˜í™˜í•˜ê³ , ê·¸ë ‡ì§€ ì•Šê³  ì…ë ¥ ë¬¸ìì—´ì˜ ëª¨ë“  ë¬¸ìê°€ ì§€ì •ëœ ë²”ìœ„ ë‚´ì— ìˆìœ¼ë©´ Trueë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

---

**Prompt 3:**  
Write a Python function to check if a number is even, and add comments in Chinese.

**Original Model Response:**
```python
# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥æ£€æŸ¥æ•°å­—æ˜¯å¦ä¸ºå¶æ•°
def is_even(number):
    # ä½¿ç”¨å–æ¨¡è¿ç®—ç¬¦ % æ¥åˆ¤æ–­æ•°å­—æ˜¯å¦èƒ½è¢«2æ•´é™¤
    # å¦‚æœä½™æ•°ä¸º0ï¼Œåˆ™è¯¥æ•°å­—æ˜¯å¶æ•°ï¼Œè¿”å›True
    if number % 2 == 0:
        return True
    # å¦åˆ™ï¼Œè¯¥æ•°å­—æ˜¯å¥‡æ•°ï¼Œè¿”å›False
    else:
        return False

# ç¤ºä¾‹è°ƒç”¨å‡½æ•°å¹¶æ‰“å°ç»“æœ
print(is_even(4))  # è¾“å‡º: True
print(is_even(7))  # è¾“å‡º: False
```

**Smoothie Qwen Response:**

```python
def is_even(num):
   """Check if a number is even.

   Args:
       num (int): The number to check.

   Returns:
       bool: True if the number is even, False otherwise.
   
   Examples:
       >>> is_even(4)
       True
       >>> is_even(5)
       False
   """
   # åˆ© modulus operator % æ¥ check 2 can divide num without remainder
   return num % 2 == 0
```

This Python function `is_even` takes an integer `num` as input and returns `True` if the number is even (i.e., divisible by 2 with no remainder), and `False` otherwise. It uses the modulus operator `%` to determine the remainder when `num` is divided by 2. If the remainder is zero, then the number is even; otherwise, it is not. The function includes a docstring that provides a brief description of its purpose, arguments, return value, and usage examples.

---

> **Observations:**
> - In general usage, the model responded in the appropriate language for the prompt without issues.
> - When deliberately prompted to generate Chinese, Smoothie Qwen effectively suppressed Chinese output in most cases, typically shifting responses to other languages or neutral content instead.
> - Core capabilities such as code generation remained intact, with no significant degradation observed after smoothing.

---

## Conclusion
**Smoothie Qwen** achieved **over 95% reduction** in unintended Chinese generation while preserving the model's core capabilities through token weight adjustment.

Key achievements:
- Basic n-gram analysis (n=2) proved highly effective
- Performance validated in realistic scenarios
- Chinese outputs were naturally replaced with other languages without loss of information.
- Core functionality maintained across all domains, including coding tasks

**Recommended configuration**: `min_scale = 0.5`, `smoothness = 10.0`

These settings provide an optimal balance between suppression and performance.
Users can adjust the n-gram size to 3 or 4 for more refined token combination suppression based on specific requirements.


## Notes
- This method modifies the model weights directly. It is recommended to validate the modelâ€™s performance after applying these changes.
- Unicode target ranges can be customized to suppress other languages or specific token patterns.
- Additional analysis methods beyond N-gram may be supported in future versions.


## References
- Logo design with â¤ï¸ by [Jë¹„ì£¼ì–¼ìŠ¤ì¿¨](https://www.jvisualschool.com/)
- [Qwen2.5 ëª¨ë¸ í™•ë¥  ì¡°ì •ì„ í†µí•´ ì¤‘êµ­ì–´ ì•ˆë‚˜ì˜¤ê²Œ í•˜ê¸°](https://www.linkedin.com/posts/jg-choi_github-workddllmforeignblock-llm-%EB%AA%A8%EB%8D%B8%EC%9D%98-activity-7306159255936540673-_RoZ) - LinkedIn

## Citation
```
@misc{ji2025smoothieqwenposthocsmoothingreduce,
      title={Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual LLMs}, 
      author={SeungWon Ji and Jungyup Lee and Jemin Kim and Sang Park and SeungJae Lee},
      year={2025},
      eprint={2507.05686},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.05686}, 
}
```
